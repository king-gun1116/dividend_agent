{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_dart_scraper.ipynb\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ”  \n",
    "1. DART `corpCode.xml`ì„ ë‚´ë ¤ë°›ì•„ ìƒì¥ì‚¬ ëª©ë¡ì„ ë¡œë“œí•˜ê³   \n",
    "2. ê° ì‚¬ë³„ë¡œ list.json í˜ì´ì§•ì„ ëŒë©° ê³µì‹œ ë©”íƒ€ì •ë³´ë¥¼ ìˆ˜ì§‘í•œ ë’¤  \n",
    "3. Seleniumì„ ì´ìš©í•´ iframe HTMLì„ ë³‘ë ¬ë¡œ ê°€ì ¸ì™€  \n",
    "4. ìµœì¢… ê²°ê³¼ë¥¼ `dividend_with_text.csv` / `dividend_with_text.jsonl`ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import requests, chardet, xmltodict, json, pandas as pd\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.exceptions import ReadTimeout\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .envì—ì„œ DART_API_KEY ë¡œë“œ\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"DART_API_KEY\")\n",
    "assert API_KEY, \"âŒ .envì— DART_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”\"\n",
    "\n",
    "# ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "DATA_DIR = \"data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# User-Agent\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "session = requests.Session()\n",
    "retry_strategy = Retry(\n",
    "    total=5,\n",
    "    backoff_factor=1,\n",
    "    status_forcelist=[429,500,502,503,504],\n",
    "    allowed_methods=[\"GET\"]\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "session.mount(\"https://\", adapter)\n",
    "session.mount(\"http://\", adapter)\n",
    "\n",
    "def _download_corp_code():\n",
    "    zip_path = os.path.join(DATA_DIR, \"corp_code.zip\")\n",
    "    xml_path = os.path.join(DATA_DIR, \"corp_code.xml\")\n",
    "    # ê¸°ì¡´ ìºì‹œ ì‚­ì œ\n",
    "    for p in (zip_path, xml_path):\n",
    "        if os.path.exists(p): os.remove(p)\n",
    "\n",
    "    url = f\"https://opendart.fss.or.kr/api/corpCode.xml?crtfc_key={API_KEY}\"\n",
    "    r = session.get(url, headers=HEADERS, timeout=30); r.raise_for_status()\n",
    "    content = r.content\n",
    "\n",
    "    # XML directly?\n",
    "    if content.lstrip().startswith(b\"<?xml\"):\n",
    "        with open(xml_path, \"wb\") as f: f.write(content)\n",
    "    else:\n",
    "        with open(zip_path, \"wb\") as f: f.write(content)\n",
    "        with zipfile.ZipFile(zip_path) as zf:\n",
    "            zf.extractall(DATA_DIR)\n",
    "        os.replace(os.path.join(DATA_DIR, \"CORPCODE.xml\"), xml_path)\n",
    "\n",
    "def load_corps() -> pd.DataFrame:\n",
    "    _download_corp_code()\n",
    "    raw = open(os.path.join(DATA_DIR, \"corp_code.xml\"), \"rb\").read()\n",
    "    enc = chardet.detect(raw)[\"encoding\"]\n",
    "    doc = xmltodict.parse(raw.decode(enc, errors=\"ignore\"))\n",
    "\n",
    "    # API í˜•íƒœì— ë§ì¶° list ë½‘ê¸°\n",
    "    if \"result\" in doc:\n",
    "        lst = doc[\"result\"].get(\"list\", [])\n",
    "    else:\n",
    "        lst = doc[\"CORPCODE\"][\"list\"][\"item\"]\n",
    "    if isinstance(lst, dict): lst = [lst]\n",
    "\n",
    "    df = pd.DataFrame(lst)[[\"corp_code\",\"corp_name\",\"stock_code\"]]\n",
    "    return df[df.stock_code.str.isdigit() & df.stock_code.str.len().eq(6)].reset_index(drop=True)\n",
    "\n",
    "def list_filings(corp_code: str, bgn=\"20130101\", end=\"20250630\") -> list:\n",
    "        all_list, fail = [], 0\n",
    "        for page in range(1, 51):\n",
    "            url = (\n",
    "                f\"https://opendart.fss.or.kr/api/list.json?\"\n",
    "                f\"crtfc_key={API_KEY}&corp_code={corp_code}\"\n",
    "                f\"&bgn_de={bgn}&end_de={end}\"\n",
    "                f\"&page_count=100&page_no={page}\"\n",
    "            )\n",
    "            try:\n",
    "                r = session.get(url, headers=HEADERS, timeout=20)\n",
    "                r.raise_for_status()\n",
    "                page_list = r.json().get(\"list\", [])\n",
    "            except ReadTimeout:\n",
    "                fail += 1\n",
    "                time.sleep(1)\n",
    "                if fail >= 3:\n",
    "                    break\n",
    "                continue\n",
    "            except:\n",
    "                break\n",
    "    \n",
    "            if not page_list:\n",
    "                break\n",
    "            all_list.extend(page_list)\n",
    "            if len(page_list) < 100:\n",
    "                break\n",
    "            time.sleep(0.1)\n",
    "        return all_list\n",
    "    \n",
    "    \n",
    "def get_report_html(rcept_no: str, wait_sec=1.0) -> tuple:\n",
    "        url = f\"https://dart.fss.or.kr/dsaf001/main.do?rcpNo={rcept_no}\"\n",
    "        opts = Options()\n",
    "        opts.add_argument(\"--headless\")\n",
    "        opts.add_argument(\"--disable-gpu\")\n",
    "        opts.add_argument(f\"user-agent={HEADERS['User-Agent']}\")\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
    "    \n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(wait_sec)\n",
    "            driver.switch_to.frame(\"ifrm\")\n",
    "            time.sleep(wait_sec)\n",
    "            html = driver.page_source\n",
    "        except:\n",
    "            html = \"\"\n",
    "        finally:\n",
    "            driver.quit()\n",
    "        return rcept_no, html\n",
    "\n",
    "    # 1) ìƒì¥ì‚¬ ëª©ë¡\n",
    "df_corps = load_corps()\n",
    "\n",
    "# 2) ëª¨ë“  ë°°ë‹¹ê³µì‹œ ë©”íƒ€ ìˆ˜ì§‘\n",
    "tasks = []\n",
    "for _, row in tqdm(df_corps.iterrows(), total=len(df_corps), desc=\"ì¢…ëª© ì²˜ë¦¬\"):\n",
    "    for f in list_filings(row.corp_code):\n",
    "        if \"ë°°ë‹¹\" in f[\"report_nm\"]:\n",
    "            tasks.append({\n",
    "                \"corp_name\": row.corp_name,\n",
    "                \"stock_code\":row.stock_code,\n",
    "                \"rcept_dt\": f[\"rcept_dt\"],\n",
    "                \"report_nm\":f[\"report_nm\"],\n",
    "                \"rcept_no\":f[\"rcept_no\"]\n",
    "            })\n",
    "\n",
    "# 3) HTML ë³‘ë ¬ ìŠ¤í¬ë˜í•‘\n",
    "results = []\n",
    "with ThreadPoolExecutor(max_workers=5) as exe:\n",
    "    futures = {exe.submit(get_report_html, t[\"rcept_no\"]):t for t in tasks}\n",
    "    for fut in tqdm(as_completed(futures), total=len(futures), desc=\"HTML ìˆ˜ì§‘\"):\n",
    "        meta = futures[fut]; rno, html = fut.result()\n",
    "        results.append({**meta, \"html\":html})\n",
    "\n",
    "# 4) DataFrameìœ¼ë¡œ ë³€í™˜ & ì €ì¥\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"data/dividend_with_text.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "with open(\"data/dividend_with_text.jsonl\",\"w\",encoding=\"utf-8\") as fw:\n",
    "    for rec in results:\n",
    "        fw.write(json.dumps(rec, ensure_ascii=False)+\"\\n\")\n",
    "\n",
    "df = pd.read_csv(\"data/dividend_with_text.csv\", encoding=\"utf-8-sig\")\n",
    "df.head()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ ì›ë³¸ ë°ì´í„°: 15,996ê±´\n",
      "âœ… ìƒì¥ ì¢…ëª© í•„í„°ë§ í›„: 15,460ê±´\n",
      "ğŸ¢ ë‚¨ì€ ê¸°ì—… ìˆ˜: 1,682ê°œ\n",
      "ğŸ’¾ ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸° ì™„ë£Œ â†’ /Users/gun/Desktop/ë¯¸ë˜ì—ì…‹ AI ê³µëª¨ì „/data/dividend_ml_ready.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import FinanceDataReader as fdr\n",
    "\n",
    "# 1. íŒŒì¼ ë¡œë“œ\n",
    "csv_path = \"/Users/gun/Desktop/ë¯¸ë˜ì—ì…‹ AI ê³µëª¨ì „/data/dividend_ml_ready.csv\"\n",
    "df = pd.read_csv(csv_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 2. ìƒì¥ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (KRX ê¸°ì¤€)\n",
    "krx_list = fdr.StockListing(\"KRX\")\n",
    "listed_codes = krx_list['Code'].astype(str).str.zfill(6).unique()\n",
    "\n",
    "# 3. í•„í„°ë§: í˜„ì¬ ìƒì¥ëœ ì¢…ëª©ë§Œ ë‚¨ê¸°ê¸°\n",
    "df[\"stock_code\"] = df[\"stock_code\"].astype(str).str.zfill(6)\n",
    "filtered_df = df[df[\"stock_code\"].isin(listed_codes)].reset_index(drop=True)\n",
    "\n",
    "# 4. ë®ì–´ì“°ê¸° ì €ì¥ (ê¸°ì¡´ íŒŒì¼ ëŒ€ì²´)\n",
    "filtered_df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 5. ìš”ì•½ ì¶œë ¥\n",
    "print(f\"ğŸ“„ ì›ë³¸ ë°ì´í„°: {len(df):,}ê±´\")\n",
    "print(f\"âœ… ìƒì¥ ì¢…ëª© í•„í„°ë§ í›„: {len(filtered_df):,}ê±´\")\n",
    "\n",
    "# âœ… ë‚¨ì€ ê³ ìœ  ê¸°ì—… ìˆ˜ í™•ì¸\n",
    "unique_firms = filtered_df[\"stock_code\"].nunique()\n",
    "print(f\"ğŸ¢ ë‚¨ì€ ê¸°ì—… ìˆ˜: {unique_firms:,}ê°œ\")\n",
    "\n",
    "print(f\"ğŸ’¾ ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸° ì™„ë£Œ â†’ {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚­ì œ ëŒ€ìƒ í–‰ ìˆ˜: 0 / ì „ì²´ 15460\n",
      "âœ… í•„í„°ë§ í›„ ë‚¨ì€ í–‰ ìˆ˜: 15460. ë®ì–´ì“°ê¸° ì™„ë£Œ: /Users/gun/Desktop/ë¯¸ë˜ì—ì…‹ AI ê³µëª¨ì „/data/dividend_ml_ready.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) MLìš© CSV ë¡œë“œ\n",
    "path = \"/Users/gun/Desktop/ë¯¸ë˜ì—ì…‹ AI ê³µëª¨ì „/data/dividend_ml_ready.csv\"\n",
    "df = pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 2) per_share_common, yield_common, total_amount ì¤‘ í•˜ë‚˜ë¼ë„ 0ì¸ ê²½ìš°\n",
    "mask_any_zero = (\n",
    "    (df.per_share_common == 0) |\n",
    "    (df.yield_common      == 0) |\n",
    "    (df.total_amount      == 0)\n",
    ")\n",
    "\n",
    "print(f\"ì‚­ì œ ëŒ€ìƒ í–‰ ìˆ˜: {mask_any_zero.sum()} / ì „ì²´ {len(df)}\")\n",
    "\n",
    "# 3) í•„í„°ë§\n",
    "df_filtered = df[~mask_any_zero].reset_index(drop=True)\n",
    "\n",
    "# 4) ì›ë³¸ íŒŒì¼ ë®ì–´ì“°ê¸° ì €ì¥\n",
    "df_filtered.to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"âœ… í•„í„°ë§ í›„ ë‚¨ì€ í–‰ ìˆ˜: {len(df_filtered)}. ë®ì–´ì“°ê¸° ì™„ë£Œ: {path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
